

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Evaluation Guide &mdash; VidSGG 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=0a84b5b7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=01f34227"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=30646c52"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API Reference" href="api.html" />
    <link rel="prev" title="Training Guide" href="training.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            VidSGG
              <img src="_static/tum-logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training Guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Evaluation Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-overview">Evaluation Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-evaluation">Basic Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#simple-evaluation-command">Simple Evaluation Command</a></li>
<li class="toctree-l3"><a class="reference internal" href="#complete-evaluation-command">Complete Evaluation Command</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-metrics">Evaluation Metrics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#recall-metrics">Recall Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#zero-shot-metrics">Zero-Shot Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#per-category-analysis">Per-Category Analysis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-procedures">Evaluation Procedures</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#standard-evaluation">Standard Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cross-dataset-evaluation">Cross-Dataset Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#temporal-evaluation">Temporal Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mode-specific-evaluation">Mode-Specific Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#predcls-evaluation">PredCLS Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sgcls-evaluation">SGCLS Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sgdet-evaluation">SGDET Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-evaluation">Advanced Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#uncertainty-evaluation">Uncertainty Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#robustness-evaluation">Robustness Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#efficiency-evaluation">Efficiency Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-analysis">Evaluation Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#statistical-significance">Statistical Significance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#error-analysis">Error Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#visualization">Visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#benchmark-comparison">Benchmark Comparison</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#standard-benchmarks">Standard Benchmarks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#leaderboard-submission">Leaderboard Submission</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#custom-evaluation">Custom Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#domain-specific-metrics">Domain-Specific Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#temporal-metrics">Temporal Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quality-assessment">Quality Assessment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-best-practices">Evaluation Best Practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#reproducibility">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-runs">Multiple Runs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#statistical-reporting">Statistical Reporting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-issues">Common Issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-debugging">Performance Debugging</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-reports">Evaluation Reports</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#automated-reports">Automated Reports</a></li>
<li class="toctree-l3"><a class="reference internal" href="#report-contents">Report Contents</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#next-steps">Next Steps</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional Information:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">VidSGG</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Evaluation Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/evaluation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="evaluation-guide">
<h1>Evaluation Guide<a class="headerlink" href="#evaluation-guide" title="Link to this heading"></a></h1>
<p>This guide covers evaluation metrics, procedures, and analysis for DLHM VidSGG models.</p>
<section id="evaluation-overview">
<h2>Evaluation Overview<a class="headerlink" href="#evaluation-overview" title="Link to this heading"></a></h2>
<p>DLHM VidSGG provides comprehensive evaluation capabilities for video scene graph generation models across different modes and datasets.</p>
<p><strong>Evaluation Modes</strong></p>
<ul class="simple">
<li><p><strong>PredCLS</strong>: Evaluate relationship prediction given ground truth objects</p></li>
<li><p><strong>SGCLS</strong>: Evaluate both object classification and relationship prediction</p></li>
<li><p><strong>SGDET</strong>: Evaluate end-to-end object detection and relationship prediction</p></li>
</ul>
</section>
<section id="basic-evaluation">
<h2>Basic Evaluation<a class="headerlink" href="#basic-evaluation" title="Link to this heading"></a></h2>
<section id="simple-evaluation-command">
<h3>Simple Evaluation Command<a class="headerlink" href="#simple-evaluation-command" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test.py<span class="w"> </span>-m<span class="w"> </span>predcls<span class="w"> </span>-datasize<span class="w"> </span>large<span class="w"> </span>-data_path<span class="w"> </span>data/action_genome<span class="w"> </span>-model_path<span class="w"> </span>output/model.pth
</pre></div>
</div>
<p>This evaluates a trained model on the Action Genome test set.</p>
</section>
<section id="complete-evaluation-command">
<h3>Complete Evaluation Command<a class="headerlink" href="#complete-evaluation-command" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-m<span class="w"> </span>predcls<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-datasize<span class="w"> </span>large<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-data_path<span class="w"> </span>data/action_genome<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-model_path<span class="w"> </span>output/sttran_predcls/checkpoint_best.tar<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-save_results<span class="w"> </span>output/evaluation_results.json
</pre></div>
</div>
</section>
</section>
<section id="evaluation-metrics">
<h2>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Link to this heading"></a></h2>
<section id="recall-metrics">
<h3>Recall Metrics<a class="headerlink" href="#recall-metrics" title="Link to this heading"></a></h3>
<p>The primary evaluation metrics for scene graph generation:</p>
<dl class="simple">
<dt><strong>Recall&#64;K</strong></dt><dd><p>Percentage of ground truth relationships that appear in top-K predictions</p>
</dd>
<dt><strong>Mean Recall&#64;K (mRecall&#64;K)</strong></dt><dd><p>Average recall across all relationship categories</p>
</dd>
</dl>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">Standard Evaluation Metrics</span><a class="headerlink" href="#id1" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="mailto:Recall&#37;&#52;&#48;10">Recall<span>&#64;</span>10</a></p></td>
<td><p>Recall considering top 10 predictions per frame</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="mailto:Recall&#37;&#52;&#48;20">Recall<span>&#64;</span>20</a></p></td>
<td><p>Recall considering top 20 predictions per frame</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="mailto:Recall&#37;&#52;&#48;50">Recall<span>&#64;</span>50</a></p></td>
<td><p>Recall considering top 50 predictions per frame</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="mailto:mRecall&#37;&#52;&#48;10">mRecall<span>&#64;</span>10</a></p></td>
<td><p>Mean recall across relationship categories (top 10)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="mailto:mRecall&#37;&#52;&#48;20">mRecall<span>&#64;</span>20</a></p></td>
<td><p>Mean recall across relationship categories (top 20)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="mailto:mRecall&#37;&#52;&#48;50">mRecall<span>&#64;</span>50</a></p></td>
<td><p>Mean recall across relationship categories (top 50)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="zero-shot-metrics">
<h3>Zero-Shot Metrics<a class="headerlink" href="#zero-shot-metrics" title="Link to this heading"></a></h3>
<p>For unseen relationship combinations:</p>
<ul class="simple">
<li><p><strong>Zero-Shot Recall&#64;K</strong>: Performance on novel object-relationship-object triplets</p></li>
<li><p><strong>Compositional Recall</strong>: Performance on new compositions of seen elements</p></li>
</ul>
</section>
<section id="per-category-analysis">
<h3>Per-Category Analysis<a class="headerlink" href="#per-category-analysis" title="Link to this heading"></a></h3>
<p>Detailed analysis for each relationship category:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example per-category results</span>
<span class="n">per_category_results</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;holding&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;recall@20&#39;</span><span class="p">:</span> <span class="mf">25.3</span><span class="p">,</span> <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="mf">18.7</span><span class="p">},</span>
    <span class="s1">&#39;sitting_on&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;recall@20&#39;</span><span class="p">:</span> <span class="mf">31.2</span><span class="p">,</span> <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="mf">22.1</span><span class="p">},</span>
    <span class="s1">&#39;looking_at&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;recall@20&#39;</span><span class="p">:</span> <span class="mf">15.8</span><span class="p">,</span> <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="mf">12.4</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="evaluation-procedures">
<h2>Evaluation Procedures<a class="headerlink" href="#evaluation-procedures" title="Link to this heading"></a></h2>
<section id="standard-evaluation">
<h3>Standard Evaluation<a class="headerlink" href="#standard-evaluation" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate all models on test set</span>
<span class="k">for</span><span class="w"> </span>model<span class="w"> </span><span class="k">in</span><span class="w"> </span>sttran<span class="w"> </span>tempura<span class="w"> </span>scenellm<span class="w"> </span>stket<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">  </span>python<span class="w"> </span>test.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>predcls<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-model_path<span class="w"> </span>output/<span class="si">${</span><span class="nv">model</span><span class="si">}</span>_predcls/checkpoint_best.tar<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-save_results<span class="w"> </span>results/<span class="si">${</span><span class="nv">model</span><span class="si">}</span>_predcls_results.json
<span class="k">done</span>
</pre></div>
</div>
</section>
<section id="cross-dataset-evaluation">
<h3>Cross-Dataset Evaluation<a class="headerlink" href="#cross-dataset-evaluation" title="Link to this heading"></a></h3>
<p>Evaluate model generalization across datasets:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train on Action Genome, test on EASG</span>
python<span class="w"> </span>test.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-m<span class="w"> </span>predcls<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-data_path<span class="w"> </span>data/EASG<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-model_path<span class="w"> </span>output/action_genome_model.pth<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-save_results<span class="w"> </span>cross_dataset_results.json
</pre></div>
</div>
</section>
<section id="temporal-evaluation">
<h3>Temporal Evaluation<a class="headerlink" href="#temporal-evaluation" title="Link to this heading"></a></h3>
<p>Analyze performance across different temporal windows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate with different temporal window sizes</span>
<span class="k">for</span><span class="w"> </span>window<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">10</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">  </span>python<span class="w"> </span>test.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>predcls<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-temporal_window<span class="w"> </span><span class="nv">$window</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-model_path<span class="w"> </span>output/model.pth
<span class="k">done</span>
</pre></div>
</div>
</section>
</section>
<section id="mode-specific-evaluation">
<h2>Mode-Specific Evaluation<a class="headerlink" href="#mode-specific-evaluation" title="Link to this heading"></a></h2>
<section id="predcls-evaluation">
<h3>PredCLS Evaluation<a class="headerlink" href="#predcls-evaluation" title="Link to this heading"></a></h3>
<p><strong>Input</strong>: Ground truth object bounding boxes and labels
<strong>Task</strong>: Predict relationships between objects</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test.py<span class="w"> </span>-m<span class="w"> </span>predcls<span class="w"> </span>-model_path<span class="w"> </span>output/sttran_predcls.pth
</pre></div>
</div>
<p><strong>Key Metrics</strong>:
* Relationship prediction accuracy
* Per-category relationship recall
* Temporal consistency</p>
</section>
<section id="sgcls-evaluation">
<h3>SGCLS Evaluation<a class="headerlink" href="#sgcls-evaluation" title="Link to this heading"></a></h3>
<p><strong>Input</strong>: Ground truth object bounding boxes
<strong>Task</strong>: Predict object labels and relationships</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test.py<span class="w"> </span>-m<span class="w"> </span>sgcls<span class="w"> </span>-model_path<span class="w"> </span>output/sttran_sgcls.pth
</pre></div>
</div>
<p><strong>Key Metrics</strong>:
* Object classification accuracy
* Relationship prediction given predicted objects
* Joint object-relationship accuracy</p>
</section>
<section id="sgdet-evaluation">
<h3>SGDET Evaluation<a class="headerlink" href="#sgdet-evaluation" title="Link to this heading"></a></h3>
<p><strong>Input</strong>: Raw video frames
<strong>Task</strong>: Detect objects and predict relationships end-to-end</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test.py<span class="w"> </span>-m<span class="w"> </span>sgdet<span class="w"> </span>-model_path<span class="w"> </span>output/sttran_sgdet.pth
</pre></div>
</div>
<p><strong>Key Metrics</strong>:
* Object detection mAP
* Relationship prediction accuracy
* End-to-end scene graph quality</p>
</section>
</section>
<section id="advanced-evaluation">
<h2>Advanced Evaluation<a class="headerlink" href="#advanced-evaluation" title="Link to this heading"></a></h2>
<section id="uncertainty-evaluation">
<h3>Uncertainty Evaluation<a class="headerlink" href="#uncertainty-evaluation" title="Link to this heading"></a></h3>
<p>For models with uncertainty estimation (e.g., Tempura):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate uncertainty calibration</span>
<span class="n">python</span> <span class="n">evaluate_uncertainty</span><span class="o">.</span><span class="n">py</span> \
  <span class="o">-</span><span class="n">model_path</span> <span class="n">output</span><span class="o">/</span><span class="n">tempura_model</span><span class="o">.</span><span class="n">pth</span> \
  <span class="o">-</span><span class="n">calibration_method</span> <span class="n">temperature_scaling</span>
</pre></div>
</div>
<p><strong>Uncertainty Metrics</strong>:
* Calibration error (ECE)
* Reliability diagrams
* Uncertainty-accuracy correlation</p>
</section>
<section id="robustness-evaluation">
<h3>Robustness Evaluation<a class="headerlink" href="#robustness-evaluation" title="Link to this heading"></a></h3>
<p>Test model robustness to various perturbations:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate with noise</span>
python<span class="w"> </span>test_robustness.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-model_path<span class="w"> </span>output/model.pth<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-noise_level<span class="w"> </span><span class="m">0</span>.1<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-noise_type<span class="w"> </span>gaussian
</pre></div>
</div>
<p><strong>Robustness Tests</strong>:
* Gaussian noise in input frames
* Occlusions and crops
* Temporal jittering
* Lighting changes</p>
</section>
<section id="efficiency-evaluation">
<h3>Efficiency Evaluation<a class="headerlink" href="#efficiency-evaluation" title="Link to this heading"></a></h3>
<p>Measure computational efficiency:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Profile model inference</span>
<span class="n">python</span> <span class="n">profile_model</span><span class="o">.</span><span class="n">py</span> \
  <span class="o">-</span><span class="n">model_path</span> <span class="n">output</span><span class="o">/</span><span class="n">model</span><span class="o">.</span><span class="n">pth</span> \
  <span class="o">-</span><span class="n">batch_size</span> <span class="mi">1</span> \
  <span class="o">-</span><span class="n">num_iterations</span> <span class="mi">100</span>
</pre></div>
</div>
<p><strong>Efficiency Metrics</strong>:
* Inference time per frame
* GPU memory usage
* FLOPs count
* Model parameters</p>
</section>
</section>
<section id="evaluation-analysis">
<h2>Evaluation Analysis<a class="headerlink" href="#evaluation-analysis" title="Link to this heading"></a></h2>
<section id="statistical-significance">
<h3>Statistical Significance<a class="headerlink" href="#statistical-significance" title="Link to this heading"></a></h3>
<p>Test statistical significance of results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Compare two models</span>
<span class="n">model1_scores</span> <span class="o">=</span> <span class="p">[</span><span class="mf">19.2</span><span class="p">,</span> <span class="mf">18.8</span><span class="p">,</span> <span class="mf">19.5</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<span class="n">model2_scores</span> <span class="o">=</span> <span class="p">[</span><span class="mf">20.1</span><span class="p">,</span> <span class="mf">19.7</span><span class="p">,</span> <span class="mf">20.3</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

<span class="n">t_stat</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">ttest_rel</span><span class="p">(</span><span class="n">model1_scores</span><span class="p">,</span> <span class="n">model2_scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P-value: </span><span class="si">{</span><span class="n">p_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="error-analysis">
<h3>Error Analysis<a class="headerlink" href="#error-analysis" title="Link to this heading"></a></h3>
<p>Analyze common failure modes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Analyze prediction errors</span>
<span class="n">python</span> <span class="n">analyze_errors</span><span class="o">.</span><span class="n">py</span> \
  <span class="o">-</span><span class="n">predictions</span> <span class="n">output</span><span class="o">/</span><span class="n">predictions</span><span class="o">.</span><span class="n">json</span> \
  <span class="o">-</span><span class="n">ground_truth</span> <span class="n">data</span><span class="o">/</span><span class="n">test_annotations</span><span class="o">.</span><span class="n">json</span> \
  <span class="o">-</span><span class="n">save_analysis</span> <span class="n">error_analysis</span><span class="o">.</span><span class="n">html</span>
</pre></div>
</div>
<p><strong>Analysis Categories</strong>:
* Frequent false positives
* Common missed relationships
* Object detection failures
* Temporal inconsistencies</p>
</section>
<section id="visualization">
<h3>Visualization<a class="headerlink" href="#visualization" title="Link to this heading"></a></h3>
<p>Generate evaluation visualizations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create evaluation plots</span>
<span class="n">python</span> <span class="n">visualize_results</span><span class="o">.</span><span class="n">py</span> \
  <span class="o">-</span><span class="n">results_dir</span> <span class="n">output</span><span class="o">/</span><span class="n">evaluation_results</span><span class="o">/</span> \
  <span class="o">-</span><span class="n">output_dir</span> <span class="n">plots</span><span class="o">/</span>
</pre></div>
</div>
<p><strong>Visualization Types</strong>:
* Recall curves
* Precision-recall plots
* Confusion matrices
* Per-category performance bars</p>
</section>
</section>
<section id="benchmark-comparison">
<h2>Benchmark Comparison<a class="headerlink" href="#benchmark-comparison" title="Link to this heading"></a></h2>
<section id="standard-benchmarks">
<h3>Standard Benchmarks<a class="headerlink" href="#standard-benchmarks" title="Link to this heading"></a></h3>
<p>Compare against established benchmarks:</p>
<table class="docutils align-default" id="id2">
<caption><span class="caption-text">Action Genome Benchmark Results</span><a class="headerlink" href="#id2" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p><a class="reference external" href="mailto:R&#37;&#52;&#48;10">R<span>&#64;</span>10</a></p></th>
<th class="head"><p><a class="reference external" href="mailto:R&#37;&#52;&#48;20">R<span>&#64;</span>20</a></p></th>
<th class="head"><p><a class="reference external" href="mailto:R&#37;&#52;&#48;50">R<span>&#64;</span>50</a></p></th>
<th class="head"><p><a class="reference external" href="mailto:mR&#37;&#52;&#48;50">mR<span>&#64;</span>50</a></p></th>
<th class="head"><p>Year</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>IMP</p></td>
<td><p>8.9</p></td>
<td><p>12.1</p></td>
<td><p>17.8</p></td>
<td><p>4.2</p></td>
<td><p>2017</p></td>
</tr>
<tr class="row-odd"><td><p>KERN</p></td>
<td><p>9.2</p></td>
<td><p>12.7</p></td>
<td><p>18.4</p></td>
<td><p>4.8</p></td>
<td><p>2019</p></td>
</tr>
<tr class="row-even"><td><p>STTran</p></td>
<td><p>14.6</p></td>
<td><p>19.2</p></td>
<td><p>26.5</p></td>
<td><p>7.8</p></td>
<td><p>2021</p></td>
</tr>
<tr class="row-odd"><td><p>Tempura</p></td>
<td><p>15.8</p></td>
<td><p>21.1</p></td>
<td><p>28.3</p></td>
<td><p>8.9</p></td>
<td><p>2022</p></td>
</tr>
</tbody>
</table>
</section>
<section id="leaderboard-submission">
<h3>Leaderboard Submission<a class="headerlink" href="#leaderboard-submission" title="Link to this heading"></a></h3>
<p>Prepare results for benchmark submission:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Format results for submission</span>
<span class="n">python</span> <span class="n">format_submission</span><span class="o">.</span><span class="n">py</span> \
  <span class="o">-</span><span class="n">predictions</span> <span class="n">output</span><span class="o">/</span><span class="n">test_predictions</span><span class="o">.</span><span class="n">json</span> \
  <span class="o">-</span><span class="n">output</span> <span class="n">submission</span><span class="o">.</span><span class="n">zip</span>
</pre></div>
</div>
</section>
</section>
<section id="custom-evaluation">
<h2>Custom Evaluation<a class="headerlink" href="#custom-evaluation" title="Link to this heading"></a></h2>
<section id="domain-specific-metrics">
<h3>Domain-Specific Metrics<a class="headerlink" href="#domain-specific-metrics" title="Link to this heading"></a></h3>
<p>Implement custom metrics for specific domains:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">custom_metric</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">):</span>
    <span class="c1"># Custom evaluation logic</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">compute_domain_specific_score</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>
</pre></div>
</div>
</section>
<section id="temporal-metrics">
<h3>Temporal Metrics<a class="headerlink" href="#temporal-metrics" title="Link to this heading"></a></h3>
<p>Evaluate temporal consistency:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">temporal_consistency</span><span class="p">(</span><span class="n">predictions</span><span class="p">):</span>
    <span class="c1"># Measure consistency across time</span>
    <span class="n">consistency_score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)):</span>
        <span class="n">consistency_score</span> <span class="o">+=</span> <span class="n">similarity</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">predictions</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">consistency_score</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="quality-assessment">
<h3>Quality Assessment<a class="headerlink" href="#quality-assessment" title="Link to this heading"></a></h3>
<p>Assess overall scene graph quality:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">scene_graph_quality</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">):</span>
    <span class="c1"># Graph-level similarity metrics</span>
    <span class="n">node_similarity</span> <span class="o">=</span> <span class="n">compute_node_similarity</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
    <span class="n">edge_similarity</span> <span class="o">=</span> <span class="n">compute_edge_similarity</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
    <span class="n">structure_similarity</span> <span class="o">=</span> <span class="n">compute_structure_similarity</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">node_similarity</span> <span class="o">+</span> <span class="n">edge_similarity</span> <span class="o">+</span> <span class="n">structure_similarity</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span>
</pre></div>
</div>
</section>
</section>
<section id="evaluation-best-practices">
<h2>Evaluation Best Practices<a class="headerlink" href="#evaluation-best-practices" title="Link to this heading"></a></h2>
<section id="reproducibility">
<h3>Reproducibility<a class="headerlink" href="#reproducibility" title="Link to this heading"></a></h3>
<p>Ensure reproducible evaluation results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seeds for consistent evaluation</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Use consistent evaluation protocols</span>
<span class="n">eval_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># For reproducibility</span>
    <span class="s1">&#39;deterministic&#39;</span><span class="p">:</span> <span class="kc">True</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="multiple-runs">
<h3>Multiple Runs<a class="headerlink" href="#multiple-runs" title="Link to this heading"></a></h3>
<p>Perform multiple evaluation runs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run evaluation multiple times with different seeds</span>
<span class="k">for</span><span class="w"> </span>seed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">42</span><span class="w"> </span><span class="m">123</span><span class="w"> </span><span class="m">456</span><span class="w"> </span><span class="m">789</span><span class="w"> </span><span class="m">999</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">  </span>python<span class="w"> </span>test.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>predcls<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-model_path<span class="w"> </span>output/model.pth<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-seed<span class="w"> </span><span class="nv">$seed</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-save_results<span class="w"> </span>results/run_<span class="si">${</span><span class="nv">seed</span><span class="si">}</span>.json
<span class="k">done</span>
</pre></div>
</div>
</section>
<section id="statistical-reporting">
<h3>Statistical Reporting<a class="headerlink" href="#statistical-reporting" title="Link to this heading"></a></h3>
<p>Report results with confidence intervals:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Calculate mean and confidence interval</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="mf">19.2</span><span class="p">,</span> <span class="mf">18.8</span><span class="p">,</span> <span class="mf">19.5</span><span class="p">,</span> <span class="mf">19.1</span><span class="p">,</span> <span class="mf">19.3</span><span class="p">]</span>
<span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">std_error</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">sem</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">ci</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">std_error</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recall@20: </span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">std_error</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> (95% CI: </span><span class="si">{</span><span class="n">ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading"></a></h2>
<section id="common-issues">
<h3>Common Issues<a class="headerlink" href="#common-issues" title="Link to this heading"></a></h3>
<dl class="simple">
<dt><strong>Low Evaluation Scores</strong></dt><dd><ul class="simple">
<li><p>Verify ground truth data format</p></li>
<li><p>Check evaluation metric implementation</p></li>
<li><p>Compare preprocessing with training</p></li>
</ul>
</dd>
<dt><strong>Inconsistent Results</strong></dt><dd><ul class="simple">
<li><p>Set random seeds for reproducibility</p></li>
<li><p>Use same data splits as training</p></li>
<li><p>Verify model loading correctly</p></li>
</ul>
</dd>
<dt><strong>Memory Issues During Evaluation</strong></dt><dd><ul class="simple">
<li><p>Reduce batch size to 1</p></li>
<li><p>Process samples sequentially</p></li>
<li><p>Clear cache between batches</p></li>
</ul>
</dd>
</dl>
</section>
<section id="performance-debugging">
<h3>Performance Debugging<a class="headerlink" href="#performance-debugging" title="Link to this heading"></a></h3>
<dl class="simple">
<dt><strong>Slow Evaluation</strong></dt><dd><ul class="simple">
<li><p>Profile bottlenecks in evaluation code</p></li>
<li><p>Optimize data loading pipeline</p></li>
<li><p>Use GPU for faster inference</p></li>
</ul>
</dd>
<dt><strong>Unexpected Results</strong></dt><dd><ul class="simple">
<li><p>Visualize predictions vs ground truth</p></li>
<li><p>Check for data leakage or preprocessing errors</p></li>
<li><p>Validate against simple baselines</p></li>
</ul>
</dd>
</dl>
</section>
</section>
<section id="evaluation-reports">
<h2>Evaluation Reports<a class="headerlink" href="#evaluation-reports" title="Link to this heading"></a></h2>
<section id="automated-reports">
<h3>Automated Reports<a class="headerlink" href="#automated-reports" title="Link to this heading"></a></h3>
<p>Generate comprehensive evaluation reports:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate evaluation report</span>
<span class="n">python</span> <span class="n">generate_report</span><span class="o">.</span><span class="n">py</span> \
  <span class="o">-</span><span class="n">results</span> <span class="n">output</span><span class="o">/</span><span class="n">evaluation_results</span><span class="o">.</span><span class="n">json</span> \
  <span class="o">-</span><span class="n">template</span> <span class="n">templates</span><span class="o">/</span><span class="n">evaluation_report</span><span class="o">.</span><span class="n">html</span> \
  <span class="o">-</span><span class="n">output</span> <span class="n">reports</span><span class="o">/</span><span class="n">model_evaluation</span><span class="o">.</span><span class="n">html</span>
</pre></div>
</div>
</section>
<section id="report-contents">
<h3>Report Contents<a class="headerlink" href="#report-contents" title="Link to this heading"></a></h3>
<p>Standard evaluation reports include:</p>
<ul class="simple">
<li><p><strong>Model Information</strong>: Architecture, parameters, training details</p></li>
<li><p><strong>Dataset Statistics</strong>: Test set size, class distribution</p></li>
<li><p><strong>Quantitative Results</strong>: All evaluation metrics with confidence intervals</p></li>
<li><p><strong>Qualitative Analysis</strong>: Visualization of predictions and failures</p></li>
<li><p><strong>Comparison</strong>: Performance relative to baselines and state-of-the-art</p></li>
</ul>
</section>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="training.html"><span class="doc">Training Guide</span></a> - Return to training with evaluation insights</p></li>
<li><p><a class="reference internal" href="models.html"><span class="doc">Models</span></a> - Understand model architectures and their evaluation characteristics</p></li>
<li><p><a class="reference internal" href="api/lib.html"><span class="doc">Library Module</span></a> - API documentation for evaluation functions</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="training.html" class="btn btn-neutral float-left" title="Training Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="api.html" class="btn btn-neutral float-right" title="API Reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, DLHM Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>