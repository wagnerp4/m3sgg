{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Comparison and Evaluation\n",
        "\n",
        "This notebook provides comprehensive evaluation and comparison of different scene graph generation models and approaches.\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Model Setup**: Initialize different scene graph generation models\n",
        "2. **Evaluation Metrics**: Implement various evaluation metrics\n",
        "3. **Comparative Analysis**: Compare model performance across different tasks\n",
        "4. **Visualization**: Create visualizations for model comparison\n",
        "5. **Benchmarking**: Run standardized benchmarks\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have the required dependencies installed:\n",
        "```bash\n",
        "pip install torch torchvision matplotlib seaborn pandas numpy scikit-learn\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Add the src directory to the path\n",
        "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "# Import m3sgg components\n",
        "from m3sgg.core.models.sttran import STTran\n",
        "from m3sgg.core.models.vlm.scene_graph_generator import VLMSceneGraphGenerator\n",
        "from m3sgg.core.datasets.action_genome import AG, cuda_collate_fn\n",
        "from m3sgg.core.config import Config\n",
        "from m3sgg.core.evaluation_recall import BasicSceneGraphEvaluator\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Evaluation Framework\n",
        "\n",
        "Let's create a comprehensive evaluation framework for comparing different models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelEvaluator:\n",
        "    \"\"\"Comprehensive evaluation framework for scene graph generation models.\"\"\"\n",
        "    \n",
        "    def __init__(self, obj_classes, rel_classes):\n",
        "        \"\"\"Initialize the evaluator.\n",
        "        \n",
        "        :param obj_classes: List of object class names\n",
        "        :param rel_classes: List of relationship class names\n",
        "        \"\"\"\n",
        "        self.obj_classes = obj_classes\n",
        "        self.rel_classes = rel_classes\n",
        "        self.results = {}\n",
        "        \n",
        "    def evaluate_model(self, model, model_name, test_data, metrics=None):\n",
        "        \"\"\"Evaluate a single model on test data.\n",
        "        \n",
        "        :param model: Model to evaluate\n",
        "        :param model_name: Name of the model\n",
        "        :param test_data: Test dataset\n",
        "        :param metrics: List of metrics to compute\n",
        "        :return: Evaluation results\n",
        "        \"\"\"\n",
        "        if metrics is None:\n",
        "            metrics = ['accuracy', 'precision', 'recall', 'f1', 'inference_time']\n",
        "        \n",
        "        print(f\"Evaluating {model_name}...\")\n",
        "        \n",
        "        results = {\n",
        "            'model_name': model_name,\n",
        "            'metrics': {},\n",
        "            'predictions': [],\n",
        "            'ground_truth': [],\n",
        "            'inference_times': []\n",
        "        }\n",
        "        \n",
        "        total_samples = len(test_data)\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        \n",
        "        for i, sample in enumerate(test_data):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            try:\n",
        "                # Get prediction from model\n",
        "                prediction = self._get_model_prediction(model, sample)\n",
        "                \n",
        "                # Get ground truth\n",
        "                ground_truth = self._get_ground_truth(sample)\n",
        "                \n",
        "                # Compute metrics\n",
        "                sample_metrics = self._compute_sample_metrics(prediction, ground_truth)\n",
        "                \n",
        "                # Update counters\n",
        "                if sample_metrics['correct']:\n",
        "                    correct_predictions += 1\n",
        "                total_predictions += 1\n",
        "                \n",
        "                # Record results\n",
        "                results['predictions'].append(prediction)\n",
        "                results['ground_truth'].append(ground_truth)\n",
        "                results['inference_times'].append(time.time() - start_time)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating sample {i}: {e}\")\n",
        "                results['inference_times'].append(0)\n",
        "        \n",
        "        # Compute overall metrics\n",
        "        results['metrics']['accuracy'] = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "        results['metrics']['inference_time'] = np.mean(results['inference_times'])\n",
        "        results['metrics']['total_samples'] = total_samples\n",
        "        results['metrics']['successful_predictions'] = total_predictions\n",
        "        \n",
        "        self.results[model_name] = results\n",
        "        return results\n",
        "    \n",
        "    def _get_model_prediction(self, model, sample):\n",
        "        \"\"\"Get prediction from model (simplified for demo).\n",
        "        \n",
        "        :param model: Model to use\n",
        "        :param sample: Input sample\n",
        "        :return: Model prediction\n",
        "        \"\"\"\n",
        "        # This is a simplified implementation\n",
        "        # In practice, you would call the actual model\n",
        "        return {\n",
        "            'objects': sample.get('objects', []),\n",
        "            'relationships': sample.get('relationships', []),\n",
        "            'confidence': 0.8  # Simulated confidence\n",
        "        }\n",
        "    \n",
        "    def _get_ground_truth(self, sample):\n",
        "        \"\"\"Get ground truth from sample.\n",
        "        \n",
        "        :param sample: Input sample\n",
        "        :return: Ground truth data\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'objects': sample.get('gt_objects', []),\n",
        "            'relationships': sample.get('gt_relationships', [])\n",
        "        }\n",
        "    \n",
        "    def _compute_sample_metrics(self, prediction, ground_truth):\n",
        "        \"\"\"Compute metrics for a single sample.\n",
        "        \n",
        "        :param prediction: Model prediction\n",
        "        :param ground_truth: Ground truth data\n",
        "        :return: Sample metrics\n",
        "        \"\"\"\n",
        "        # Simplified metric computation\n",
        "        pred_objects = set(prediction.get('objects', []))\n",
        "        gt_objects = set(ground_truth.get('objects', []))\n",
        "        \n",
        "        pred_rels = set(prediction.get('relationships', []))\n",
        "        gt_rels = set(ground_truth.get('relationships', []))\n",
        "        \n",
        "        # Object accuracy\n",
        "        object_accuracy = len(pred_objects.intersection(gt_objects)) / len(gt_objects) if gt_objects else 0\n",
        "        \n",
        "        # Relationship accuracy\n",
        "        rel_accuracy = len(pred_rels.intersection(gt_rels)) / len(gt_rels) if gt_rels else 0\n",
        "        \n",
        "        # Overall correctness (simplified)\n",
        "        correct = object_accuracy > 0.5 and rel_accuracy > 0.5\n",
        "        \n",
        "        return {\n",
        "            'object_accuracy': object_accuracy,\n",
        "            'relationship_accuracy': rel_accuracy,\n",
        "            'correct': correct\n",
        "        }\n",
        "    \n",
        "    def compare_models(self, model_names=None):\n",
        "        \"\"\"Compare multiple models.\n",
        "        \n",
        "        :param model_names: List of model names to compare\n",
        "        :return: Comparison results\n",
        "        \"\"\"\n",
        "        if model_names is None:\n",
        "            model_names = list(self.results.keys())\n",
        "        \n",
        "        comparison = {\n",
        "            'models': model_names,\n",
        "            'metrics': {},\n",
        "            'rankings': {}\n",
        "        }\n",
        "        \n",
        "        # Extract metrics for comparison\n",
        "        for metric in ['accuracy', 'inference_time']:\n",
        "            comparison['metrics'][metric] = {}\n",
        "            for model_name in model_names:\n",
        "                if model_name in self.results:\n",
        "                    comparison['metrics'][metric][model_name] = self.results[model_name]['metrics'].get(metric, 0)\n",
        "        \n",
        "        # Create rankings\n",
        "        for metric in comparison['metrics']:\n",
        "            sorted_models = sorted(\n",
        "                comparison['metrics'][metric].items(),\n",
        "                key=lambda x: x[1],\n",
        "                reverse=(metric != 'inference_time')  # Lower is better for inference time\n",
        "            )\n",
        "            comparison['rankings'][metric] = [model for model, _ in sorted_models]\n",
        "        \n",
        "        return comparison\n",
        "    \n",
        "    def visualize_comparison(self, comparison, save_path=None):\n",
        "        \"\"\"Visualize model comparison results.\n",
        "        \n",
        "        :param comparison: Comparison results\n",
        "        :param save_path: Path to save the plot\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Accuracy comparison\n",
        "        models = comparison['models']\n",
        "        accuracies = [comparison['metrics']['accuracy'].get(model, 0) for model in models]\n",
        "        \n",
        "        axes[0].bar(models, accuracies, color='skyblue', alpha=0.7)\n",
        "        axes[0].set_title('Model Accuracy Comparison')\n",
        "        axes[0].set_ylabel('Accuracy')\n",
        "        axes[0].set_ylim(0, 1)\n",
        "        axes[0].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Inference time comparison\n",
        "        times = [comparison['metrics']['inference_time'].get(model, 0) for model in models]\n",
        "        \n",
        "        axes[1].bar(models, times, color='lightcoral', alpha=0.7)\n",
        "        axes[1].set_title('Model Inference Time Comparison')\n",
        "        axes[1].set_ylabel('Inference Time (seconds)')\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        \n",
        "        plt.show()\n",
        "    \n",
        "    def export_results(self, output_path=\"model_evaluation_results.json\"):\n",
        "        \"\"\"Export evaluation results.\n",
        "        \n",
        "        :param output_path: Path to save results\n",
        "        :return: Exported data\n",
        "        \"\"\"\n",
        "        export_data = {\n",
        "            'metadata': {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'total_models': len(self.results),\n",
        "                'object_classes': self.obj_classes,\n",
        "                'relationship_classes': self.rel_classes\n",
        "            },\n",
        "            'results': self.results\n",
        "        }\n",
        "        \n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(export_data, f, indent=2)\n",
        "        \n",
        "        print(f\"Results exported to {output_path}\")\n",
        "        return export_data\n",
        "\n",
        "print(\"ModelEvaluator class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup and Test Data\n",
        "\n",
        "Let's set up the evaluation environment and create test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = Config()\n",
        "config.data_path = \"../data/action_genome\"  # Adjust path as needed\n",
        "config.mode = \"sgdet\"  # Scene graph detection mode\n",
        "config.datasize = 100  # Use larger dataset for evaluation\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset\n",
        "try:\n",
        "    dataset = AG(\n",
        "        mode=\"test\",\n",
        "        datasize=config.datasize,\n",
        "        data_path=config.data_path,\n",
        "        filter_nonperson_box_frame=True,\n",
        "        filter_small_box=False if config.mode == \"predcls\" else True,\n",
        "    )\n",
        "    \n",
        "    obj_classes = dataset.obj_classes\n",
        "    rel_classes = dataset.rel_classes\n",
        "    \n",
        "    print(f\"Dataset loaded successfully!\")\n",
        "    print(f\"Object classes: {len(obj_classes)}\")\n",
        "    print(f\"Relationship classes: {len(rel_classes)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Using default classes for demonstration\")\n",
        "    obj_classes = [\"person\", \"car\", \"bicycle\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\"]\n",
        "    rel_classes = [\"above\", \"below\", \"in front of\", \"behind\", \"next to\", \"on\", \"in\", \"at\", \"with\", \"near\"]\n",
        "\n",
        "# Create test data (simplified for demo)\n",
        "test_data = []\n",
        "for i in range(10):  # Create 10 test samples\n",
        "    sample = {\n",
        "        'objects': obj_classes[:3],  # First 3 object classes\n",
        "        'relationships': [(obj_classes[0], rel_classes[0], obj_classes[1])],\n",
        "        'gt_objects': obj_classes[:3],\n",
        "        'gt_relationships': [(obj_classes[0], rel_classes[0], obj_classes[1])]\n",
        "    }\n",
        "    test_data.append(sample)\n",
        "\n",
        "print(f\"Created {len(test_data)} test samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Evaluation\n",
        "\n",
        "Now let's evaluate different models using our evaluation framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(obj_classes, rel_classes)\n",
        "\n",
        "# Define models to evaluate\n",
        "models_to_evaluate = [\n",
        "    {\n",
        "        'name': 'STTran',\n",
        "        'model': None,  # Would be actual STTran model\n",
        "        'description': 'Spatial-Temporal Transformer baseline'\n",
        "    },\n",
        "    {\n",
        "        'name': 'VLM-SGG',\n",
        "        'model': None,  # Would be actual VLM model\n",
        "        'description': 'Vision-Language Model for scene graph generation'\n",
        "    },\n",
        "    {\n",
        "        'name': 'Tempura',\n",
        "        'model': None,  # Would be actual Tempura model\n",
        "        'description': 'Temporal relationship modeling with uncertainty'\n",
        "    },\n",
        "    {\n",
        "        'name': 'SceneLLM',\n",
        "        'model': None,  # Would be actual SceneLLM model\n",
        "        'description': 'Large language model integration'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Models to evaluate:\")\n",
        "for model_info in models_to_evaluate:\n",
        "    print(f\"- {model_info['name']}: {model_info['description']}\")\n",
        "\n",
        "# Evaluate each model\n",
        "print(\"\\\\nStarting model evaluation...\")\n",
        "for model_info in models_to_evaluate:\n",
        "    try:\n",
        "        # Simulate model evaluation\n",
        "        results = evaluator.evaluate_model(\n",
        "            model=model_info['model'],\n",
        "            model_name=model_info['name'],\n",
        "            test_data=test_data\n",
        "        )\n",
        "        print(f\"✓ {model_info['name']} evaluation completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error evaluating {model_info['name']}: {e}\")\n",
        "\n",
        "print(\"\\\\nModel evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Comparison and Visualization\n",
        "\n",
        "Let's compare the evaluated models and create visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "print(\"Comparing models...\")\n",
        "comparison = evaluator.compare_models()\n",
        "\n",
        "if comparison['models']:\n",
        "    print(\"\\\\nModel Comparison Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Display accuracy results\n",
        "    print(\"\\\\nAccuracy Rankings:\")\n",
        "    for i, model in enumerate(comparison['rankings']['accuracy'], 1):\n",
        "        accuracy = comparison['metrics']['accuracy'].get(model, 0)\n",
        "        print(f\"{i}. {model}: {accuracy:.3f}\")\n",
        "    \n",
        "    # Display inference time results\n",
        "    print(\"\\\\nInference Time Rankings (lower is better):\")\n",
        "    for i, model in enumerate(comparison['rankings']['inference_time'], 1):\n",
        "        time_val = comparison['metrics']['inference_time'].get(model, 0)\n",
        "        print(f\"{i}. {model}: {time_val:.3f}s\")\n",
        "    \n",
        "    # Create visualizations\n",
        "    print(\"\\\\nCreating visualizations...\")\n",
        "    evaluator.visualize_comparison(comparison, save_path=\"model_comparison.png\")\n",
        "    \n",
        "    # Create detailed comparison table\n",
        "    print(\"\\\\nDetailed Comparison Table:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Model':<15} {'Accuracy':<10} {'Inference Time':<15} {'Rank (Acc)':<12} {'Rank (Time)':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for model in comparison['models']:\n",
        "        accuracy = comparison['metrics']['accuracy'].get(model, 0)\n",
        "        time_val = comparison['metrics']['inference_time'].get(model, 0)\n",
        "        acc_rank = comparison['rankings']['accuracy'].index(model) + 1\n",
        "        time_rank = comparison['rankings']['inference_time'].index(model) + 1\n",
        "        \n",
        "        print(f\"{model:<15} {accuracy:<10.3f} {time_val:<15.3f} {acc_rank:<12} {time_rank:<12}\")\n",
        "    \n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "else:\n",
        "    print(\"No models evaluated successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Export Results and Summary\n",
        "\n",
        "Finally, let's export the evaluation results and provide a comprehensive summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results\n",
        "print(\"Exporting evaluation results...\")\n",
        "export_data = evaluator.export_results(\"model_evaluation_results.json\")\n",
        "\n",
        "print(f\"Exported results for {len(export_data['results'])} models\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\\\nModel Evaluation Summary:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total models evaluated: {len(export_data['results'])}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n",
        "print(f\"Object classes: {len(obj_classes)}\")\n",
        "print(f\"Relationship classes: {len(rel_classes)}\")\n",
        "\n",
        "if comparison['models']:\n",
        "    print(\"\\\\nBest Performing Models:\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Best accuracy\n",
        "    best_acc_model = comparison['rankings']['accuracy'][0]\n",
        "    best_acc_value = comparison['metrics']['accuracy'].get(best_acc_model, 0)\n",
        "    print(f\"Best Accuracy: {best_acc_model} ({best_acc_value:.3f})\")\n",
        "    \n",
        "    # Best inference time\n",
        "    best_time_model = comparison['rankings']['inference_time'][0]\n",
        "    best_time_value = comparison['metrics']['inference_time'].get(best_time_model, 0)\n",
        "    print(f\"Best Inference Time: {best_time_model} ({best_time_value:.3f}s)\")\n",
        "    \n",
        "    # Overall best (balanced)\n",
        "    print(\"\\\\nOverall Rankings (balanced accuracy and speed):\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Calculate balanced scores\n",
        "    balanced_scores = {}\n",
        "    for model in comparison['models']:\n",
        "        accuracy = comparison['metrics']['accuracy'].get(model, 0)\n",
        "        time_val = comparison['metrics']['inference_time'].get(model, 0)\n",
        "        \n",
        "        # Normalize scores (higher is better for both)\n",
        "        acc_score = accuracy\n",
        "        time_score = 1.0 / (time_val + 1e-6)  # Avoid division by zero\n",
        "        \n",
        "        # Weighted average (equal weights)\n",
        "        balanced_score = 0.5 * acc_score + 0.5 * time_score\n",
        "        balanced_scores[model] = balanced_score\n",
        "    \n",
        "    # Sort by balanced score\n",
        "    sorted_balanced = sorted(balanced_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    for i, (model, score) in enumerate(sorted_balanced, 1):\n",
        "        print(f\"{i}. {model}: {score:.3f}\")\n",
        "\n",
        "print(\"\\\\nEvaluation completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provided a comprehensive evaluation and comparison framework for scene graph generation models:\n",
        "\n",
        "1. **Evaluation Framework**: Created a flexible `ModelEvaluator` class for standardized evaluation\n",
        "2. **Model Setup**: Configured multiple models for comparison\n",
        "3. **Metrics**: Implemented accuracy, inference time, and other performance metrics\n",
        "4. **Visualization**: Created comparative visualizations and rankings\n",
        "5. **Export**: Saved results for further analysis\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Standardized Evaluation**: Consistent evaluation across different models\n",
        "- **Multiple Metrics**: Accuracy, inference time, and balanced scoring\n",
        "- **Visualization**: Clear comparison charts and tables\n",
        "- **Flexible Framework**: Easy to add new models and metrics\n",
        "- **Export Functionality**: JSON export for further analysis\n",
        "\n",
        "### Evaluation Metrics\n",
        "\n",
        "- **Accuracy**: Overall correctness of predictions\n",
        "- **Inference Time**: Speed of model execution\n",
        "- **Balanced Score**: Weighted combination of accuracy and speed\n",
        "- **Rankings**: Comparative performance across models\n",
        "\n",
        "### Model Comparison\n",
        "\n",
        "The framework allows for easy comparison of:\n",
        "- Traditional models (STTran, Tempura)\n",
        "- VLM-based models (VLM-SGG, SceneLLM)\n",
        "- Different architectures and approaches\n",
        "- Performance vs. speed trade-offs\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Add more sophisticated metrics (ROUGE, BLEU, etc.)\n",
        "- Implement domain-specific evaluations\n",
        "- Add temporal consistency metrics\n",
        "- Create interactive visualizations\n",
        "- Integrate with automated benchmarking\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Model Loading**: Ensure all models are properly initialized\n",
        "2. **Data Issues**: Verify test data format and availability\n",
        "3. **Memory Issues**: Reduce batch size or use smaller models\n",
        "4. **Evaluation Errors**: Check model compatibility and data format\n",
        "5. **Visualization Issues**: Ensure matplotlib and seaborn are installed\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
