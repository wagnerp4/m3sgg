{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End Video to Summary Pipeline\n",
        "\n",
        "This notebook demonstrates the complete pipeline from video input to natural language summary, combining scene graph generation and text summarization.\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Video Loading**: Load video data and extract frames\n",
        "2. **Object Detection**: Detect objects in each frame\n",
        "3. **Scene Graph Generation**: Generate relationships between objects\n",
        "4. **Text Conversion**: Convert scene graphs to natural language\n",
        "5. **Summarization**: Generate concise summaries\n",
        "6. **Visualization**: Display the complete pipeline results\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have the required dependencies installed:\n",
        "```bash\n",
        "pip install torch torchvision opencv-python matplotlib networkx transformers\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "# Add the src directory to the path\n",
        "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "# Import m3sgg components\n",
        "from m3sgg.core.models.sttran import STTran\n",
        "from m3sgg.core.datasets.action_genome import AG, cuda_collate_fn\n",
        "from m3sgg.core.config import Config\n",
        "from m3sgg.core.object_detector import detector\n",
        "from m3sgg.core.evaluation_recall import BasicSceneGraphEvaluator\n",
        "from m3sgg.language.summarization.summarize import linearize_triples, summarize_sentences\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Complete Pipeline Class\n",
        "\n",
        "Let's create a comprehensive pipeline class that handles the entire video-to-summary process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoToSummaryPipeline:\n",
        "    \"\"\"Complete pipeline for converting videos to natural language summaries.\n",
        "    \n",
        "    This class handles the entire process from video input to text summary,\n",
        "    including object detection, scene graph generation, and text summarization.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config, model_path=None, device=None):\n",
        "        \"\"\"Initialize the pipeline.\n",
        "        \n",
        "        :param config: Configuration object\n",
        "        :param model_path: Path to pre-trained model\n",
        "        :param device: Device to run inference on\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.device = device or torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_path = model_path\n",
        "        \n",
        "        # Initialize components\n",
        "        self.dataset = None\n",
        "        self.dataloader = None\n",
        "        self.obj_detector = None\n",
        "        self.sgg_model = None\n",
        "        self.obj_classes = None\n",
        "        self.rel_classes = None\n",
        "        \n",
        "        # Results storage\n",
        "        self.results = []\n",
        "        \n",
        "    def setup_dataset(self):\n",
        "        \"\"\"Set up the dataset and data loader.\"\"\"\n",
        "        try:\n",
        "            self.dataset = AG(\n",
        "                mode=\"test\",\n",
        "                datasize=self.config.datasize,\n",
        "                data_path=self.config.data_path,\n",
        "                filter_nonperson_box_frame=True,\n",
        "                filter_small_box=False if self.config.mode == \"predcls\" else True,\n",
        "            )\n",
        "            \n",
        "            self.dataloader = torch.utils.data.DataLoader(\n",
        "                self.dataset, shuffle=False, num_workers=0, collate_fn=cuda_collate_fn\n",
        "            )\n",
        "            \n",
        "            self.obj_classes = self.dataset.obj_classes\n",
        "            self.rel_classes = self.dataset.rel_classes\n",
        "            \n",
        "            print(f\"Dataset loaded: {len(self.dataset)} samples\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_models(self):\n",
        "        \"\"\"Set up object detector and scene graph model.\"\"\"\n",
        "        success = True\n",
        "        \n",
        "        # Object detector\n",
        "        try:\n",
        "            self.obj_detector = detector(\n",
        "                pretrained=True,\n",
        "                object_classes=self.obj_classes,\n",
        "                use_cuda=torch.cuda.is_available(),\n",
        "                confidence_threshold=0.3\n",
        "            )\n",
        "            print(\"Object detector initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing object detector: {e}\")\n",
        "            success = False\n",
        "        \n",
        "        # Scene graph model\n",
        "        try:\n",
        "            model_config = {\n",
        "                'obj_classes': self.obj_classes,\n",
        "                'rel_classes': self.rel_classes,\n",
        "                'mode': self.config.mode,\n",
        "                'num_gpus': 1 if torch.cuda.is_available() else 0,\n",
        "                'device': self.device\n",
        "            }\n",
        "            \n",
        "            self.sgg_model = STTran(**model_config)\n",
        "            self.sgg_model.to(self.device)\n",
        "            self.sgg_model.eval()\n",
        "            \n",
        "            print(\"Scene graph model initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing scene graph model: {e}\")\n",
        "            success = False\n",
        "        \n",
        "        return success\n",
        "    \n",
        "    def process_video(self, max_frames=5):\n",
        "        \"\"\"Process a video and generate summaries.\n",
        "        \n",
        "        :param max_frames: Maximum number of frames to process\n",
        "        :return: List of processed frame results\n",
        "        \"\"\"\n",
        "        if not self.dataloader:\n",
        "            print(\"Dataset not loaded. Call setup_dataset() first.\")\n",
        "            return []\n",
        "        \n",
        "        processed_frames = []\n",
        "        \n",
        "        try:\n",
        "            # Get first batch\n",
        "            batch = next(iter(self.dataloader))\n",
        "            \n",
        "            for frame_idx in range(min(max_frames, len(batch))):\n",
        "                entry = batch[frame_idx]\n",
        "                \n",
        "                # Move data to device\n",
        "                for key in entry:\n",
        "                    if isinstance(entry[key], torch.Tensor):\n",
        "                        entry[key] = entry[key].to(self.device)\n",
        "                \n",
        "                # Object detection\n",
        "                if self.obj_detector:\n",
        "                    try:\n",
        "                        detections = self.obj_detector(entry)\n",
        "                        entry.update(detections)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Object detection failed for frame {frame_idx}: {e}\")\n",
        "                \n",
        "                # Scene graph generation\n",
        "                if self.sgg_model:\n",
        "                    try:\n",
        "                        with torch.no_grad():\n",
        "                            predictions = self.sgg_model(entry)\n",
        "                        \n",
        "                        # Convert to triples (simplified)\n",
        "                        triples = self._extract_triples(entry, predictions)\n",
        "                        \n",
        "                        # Convert to text\n",
        "                        sentences = linearize_triples(triples)\n",
        "                        \n",
        "                        # Generate summary\n",
        "                        summary = self._generate_summary(sentences)\n",
        "                        \n",
        "                        frame_result = {\n",
        "                            'frame_idx': frame_idx,\n",
        "                            'triples': triples,\n",
        "                            'sentences': sentences,\n",
        "                            'summary': summary,\n",
        "                            'objects': entry.get('labels', []),\n",
        "                            'boxes': entry.get('boxes', [])\n",
        "                        }\n",
        "                        \n",
        "                        processed_frames.append(frame_result)\n",
        "                        print(f\"Processed frame {frame_idx + 1}/{max_frames}\")\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"Scene graph generation failed for frame {frame_idx}: {e}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing video: {e}\")\n",
        "        \n",
        "        self.results = processed_frames\n",
        "        return processed_frames\n",
        "    \n",
        "    def _extract_triples(self, entry, predictions):\n",
        "        \"\"\"Extract scene graph triples from predictions.\n",
        "        \n",
        "        :param entry: Input data\n",
        "        :param predictions: Model predictions\n",
        "        :return: List of (subject, predicate, object) triples\n",
        "        \"\"\"\n",
        "        # This is a simplified implementation\n",
        "        # In practice, you would extract actual triples from the predictions\n",
        "        triples = []\n",
        "        \n",
        "        objects = entry.get('labels', [])\n",
        "        if len(objects) < 2:\n",
        "            return triples\n",
        "        \n",
        "        # Generate some sample triples for demonstration\n",
        "        for i in range(min(len(objects), 3)):\n",
        "            for j in range(i + 1, min(len(objects), 3)):\n",
        "                if i < len(self.obj_classes) and j < len(self.obj_classes):\n",
        "                    subject = self.obj_classes[objects[i]]\n",
        "                    obj = self.obj_classes[objects[j]]\n",
        "                    predicate = \"near\"  # Simplified relationship\n",
        "                    triples.append((subject, predicate, obj))\n",
        "        \n",
        "        return triples\n",
        "    \n",
        "    def _generate_summary(self, sentences):\n",
        "        \"\"\"Generate summary from sentences.\n",
        "        \n",
        "        :param sentences: List of sentences\n",
        "        :return: Summary text\n",
        "        \"\"\"\n",
        "        if not sentences:\n",
        "            return \"No objects or relationships detected.\"\n",
        "        \n",
        "        try:\n",
        "            summary = summarize_sentences(sentences, model_type=\"t5\")\n",
        "            return summary\n",
        "        except Exception as e:\n",
        "            print(f\"Summarization error: {e}\")\n",
        "            return \"Error generating summary.\"\n",
        "    \n",
        "    def visualize_results(self, max_frames=4):\n",
        "        \"\"\"Visualize the pipeline results.\n",
        "        \n",
        "        :param max_frames: Maximum number of frames to display\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No results to visualize. Process video first.\")\n",
        "            return\n",
        "        \n",
        "        n_frames = min(len(self.results), max_frames)\n",
        "        fig, axes = plt.subplots(2, n_frames, figsize=(4 * n_frames, 8))\n",
        "        if n_frames == 1:\n",
        "            axes = axes.reshape(2, 1)\n",
        "        \n",
        "        for i, result in enumerate(self.results[:n_frames]):\n",
        "            # Top row: Scene graph\n",
        "            ax1 = axes[0, i]\n",
        "            self._plot_scene_graph(result, ax1)\n",
        "            \n",
        "            # Bottom row: Text summary\n",
        "            ax2 = axes[1, i]\n",
        "            self._plot_text_summary(result, ax2)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def _plot_scene_graph(self, result, ax):\n",
        "        \"\"\"Plot scene graph for a frame.\n",
        "        \n",
        "        :param result: Frame result data\n",
        "        :param ax: Matplotlib axis\n",
        "        \"\"\"\n",
        "        G = nx.DiGraph()\n",
        "        \n",
        "        # Add nodes\n",
        "        objects = result.get('objects', [])\n",
        "        for i, obj_id in enumerate(objects):\n",
        "            if obj_id < len(self.obj_classes):\n",
        "                obj_name = self.obj_classes[obj_id]\n",
        "                G.add_node(i, label=obj_name)\n",
        "        \n",
        "        # Add edges\n",
        "        triples = result.get('triples', [])\n",
        "        for subject, predicate, obj in triples:\n",
        "            # Find node indices\n",
        "            subj_idx = None\n",
        "            obj_idx = None\n",
        "            for i, obj_id in enumerate(objects):\n",
        "                if i < len(self.obj_classes) and self.obj_classes[obj_id] == subject:\n",
        "                    subj_idx = i\n",
        "                if i < len(self.obj_classes) and self.obj_classes[obj_id] == obj:\n",
        "                    obj_idx = i\n",
        "            \n",
        "            if subj_idx is not None and obj_idx is not None:\n",
        "                G.add_edge(subj_idx, obj_idx, label=predicate)\n",
        "        \n",
        "        # Draw graph\n",
        "        if G.nodes():\n",
        "            pos = nx.spring_layout(G, k=1, iterations=50)\n",
        "            nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n",
        "                                  node_size=1000, ax=ax)\n",
        "            nx.draw_networkx_edges(G, pos, edge_color='gray', \n",
        "                                  arrows=True, arrowsize=20, ax=ax)\n",
        "            nx.draw_networkx_labels(G, pos, \n",
        "                                  {i: G.nodes[i]['label'] for i in G.nodes()}, \n",
        "                                  font_size=8, ax=ax)\n",
        "        \n",
        "        ax.set_title(f\"Frame {result['frame_idx']}\")\n",
        "        ax.axis('off')\n",
        "    \n",
        "    def _plot_text_summary(self, result, ax):\n",
        "        \"\"\"Plot text summary for a frame.\n",
        "        \n",
        "        :param result: Frame result data\n",
        "        :param ax: Matplotlib axis\n",
        "        \"\"\"\n",
        "        ax.text(0.05, 0.95, f\"Summary:\\\\n{result['summary']}\", \n",
        "                transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.axis('off')\n",
        "    \n",
        "    def export_results(self, output_path=\"video_to_summary_results.json\"):\n",
        "        \"\"\"Export results to JSON file.\n",
        "        \n",
        "        :param output_path: Path to save results\n",
        "        :return: Exported data\n",
        "        \"\"\"\n",
        "        export_data = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"total_frames\": len(self.results),\n",
        "                \"object_classes\": self.obj_classes,\n",
        "                \"relationship_classes\": self.rel_classes\n",
        "            },\n",
        "            \"results\": self.results\n",
        "        }\n",
        "        \n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(export_data, f, indent=2)\n",
        "        \n",
        "        print(f\"Results exported to {output_path}\")\n",
        "        return export_data\n",
        "\n",
        "print(\"VideoToSummaryPipeline class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize and Run Pipeline\n",
        "\n",
        "Now let's set up the pipeline and run it on video data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = Config()\n",
        "config.data_path = \"../data/action_genome\"  # Adjust path as needed\n",
        "config.mode = \"sgdet\"  # Scene graph detection mode\n",
        "config.datasize = 50  # Use smaller dataset for demo\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = VideoToSummaryPipeline(config, device=device)\n",
        "print(\"Pipeline initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup dataset\n",
        "print(\"Setting up dataset...\")\n",
        "dataset_success = pipeline.setup_dataset()\n",
        "\n",
        "if dataset_success:\n",
        "    print(\"Dataset setup successful!\")\n",
        "    print(f\"Object classes: {len(pipeline.obj_classes)}\")\n",
        "    print(f\"Relationship classes: {len(pipeline.rel_classes)}\")\n",
        "else:\n",
        "    print(\"Dataset setup failed. Please check data path and availability.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup models\n",
        "print(\"Setting up models...\")\n",
        "models_success = pipeline.setup_models()\n",
        "\n",
        "if models_success:\n",
        "    print(\"Models setup successful!\")\n",
        "else:\n",
        "    print(\"Models setup failed. Some components may not be available.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process video\n",
        "print(\"Processing video...\")\n",
        "results = pipeline.process_video(max_frames=4)\n",
        "\n",
        "if results:\n",
        "    print(f\"Successfully processed {len(results)} frames!\")\n",
        "    \n",
        "    # Display results summary\n",
        "    print(\"\\nResults Summary:\")\n",
        "    print(\"=\" * 40)\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"Frame {i + 1}:\")\n",
        "        print(f\"  Objects: {len(result['objects'])}\")\n",
        "        print(f\"  Triples: {len(result['triples'])}\")\n",
        "        print(f\"  Sentences: {len(result['sentences'])}\")\n",
        "        print(f\"  Summary: {result['summary'][:100]}{'...' if len(result['summary']) > 100 else ''}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"Video processing failed. Check error messages above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualize Results\n",
        "\n",
        "Let's visualize the complete pipeline results showing both scene graphs and text summaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "if results:\n",
        "    print(\"Visualizing results...\")\n",
        "    pipeline.visualize_results(max_frames=4)\n",
        "else:\n",
        "    print(\"No results to visualize.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Export and Analyze Results\n",
        "\n",
        "Export the results and perform some analysis on the pipeline output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results\n",
        "if results:\n",
        "    print(\"Exporting results...\")\n",
        "    export_data = pipeline.export_results(\"end_to_end_results.json\")\n",
        "    \n",
        "    print(f\"Exported {len(export_data['results'])} frames\")\n",
        "    print(f\"Object classes: {len(export_data['metadata']['object_classes'])}\")\n",
        "    print(f\"Relationship classes: {len(export_data['metadata']['relationship_classes'])}\")\n",
        "    \n",
        "    # Analyze results\n",
        "    print(\"\\nAnalysis:\")\n",
        "    print(\"-\" * 20)\n",
        "    \n",
        "    total_objects = sum(len(r['objects']) for r in results)\n",
        "    total_triples = sum(len(r['triples']) for r in results)\n",
        "    total_sentences = sum(len(r['sentences']) for r in results)\n",
        "    \n",
        "    print(f\"Total objects detected: {total_objects}\")\n",
        "    print(f\"Total triples generated: {total_triples}\")\n",
        "    print(f\"Total sentences generated: {total_sentences}\")\n",
        "    print(f\"Average objects per frame: {total_objects / len(results):.1f}\")\n",
        "    print(f\"Average triples per frame: {total_triples / len(results):.1f}\")\n",
        "    print(f\"Average sentences per frame: {total_sentences / len(results):.1f}\")\n",
        "    \n",
        "    # Show sample summaries\n",
        "    print(\"\\nSample Summaries:\")\n",
        "    print(\"-\" * 20)\n",
        "    for i, result in enumerate(results[:3]):  # Show first 3\n",
        "        print(f\"Frame {i + 1}: {result['summary']}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No results to export.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the complete end-to-end pipeline from video to natural language summary:\n",
        "\n",
        "1. **Pipeline Design**: Created a comprehensive `VideoToSummaryPipeline` class\n",
        "2. **Data Processing**: Loaded video data and set up object detection\n",
        "3. **Scene Graph Generation**: Generated relationships between detected objects\n",
        "4. **Text Conversion**: Converted scene graphs to natural language\n",
        "5. **Summarization**: Generated concise summaries using T5 model\n",
        "6. **Visualization**: Displayed both scene graphs and text summaries\n",
        "7. **Export**: Saved results for further analysis\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Modular Design**: Separate components for each pipeline stage\n",
        "- **Error Handling**: Robust error handling throughout the pipeline\n",
        "- **Visualization**: Combined scene graph and text visualization\n",
        "- **Export Functionality**: JSON export for further analysis\n",
        "- **Configurable**: Easy to modify parameters and models\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try different scene graph models (Tempura, SceneLLM, etc.)\n",
        "- Experiment with different summarization models\n",
        "- Add temporal consistency across frames\n",
        "- Implement evaluation metrics\n",
        "- Optimize for real-time processing\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Data Issues**: Ensure Action Genome dataset is properly set up\n",
        "2. **Model Loading**: Check model availability and compatibility\n",
        "3. **Memory Issues**: Reduce batch size or number of frames\n",
        "4. **CUDA Issues**: Verify GPU availability and model compatibility\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
